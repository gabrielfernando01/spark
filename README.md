![](https://raw.githubusercontent.com/gabrielfernando01/spark/master/image/cover.png)
  
# Instalaci√≥n de Spark ‚≠ê en GNU/Linux Debian
## Introduction

[web_spark](https://spark.apache.org/) 

Apache Spark ‚≠ê is a multi-languange engine for executing data engineering, data science, and machine learning on single-node machines or cluster.

Apache Spark ‚≠ê es un framework (conjunto de herramientas) usadas en el entorno de agrupaci√≥n computacional para el **an√°lisis de big data**.

En este turorial, mostrare como **instalar Spark en una distribuci√≥n Debian o Ubuntu**. La guia muestra como inicializar un servidor maestro (master) y trabajador (slave), y como cargar las terminales (shells) de Scala y Python. As√≠ como los principales comandos de Spark.

### Resources

+ üíª ordenador: 16 RAM, 250 GB.
+ üêß OS: Kubunu 24.04.2
+ üß† Kernel: 6.8.0-52-generic
+ ‚òï javaJDK: openjdk 11.0.26
+ üõ£Ô∏è $JAVA_HOME: /usr/lib/jvm/java-11-openjdk-amd64
+ üü• scala version: 2.13.8
+ üõ£Ô∏è $SCALA_HOME: /usr/local/share/scala
+ ü™∂ maven version: 3.8.7
+ üõ£Ô∏è path maven: /usr/share/maven
+ üîå sbt verison: 1.10.7
+ ‚≠ê spark version: 3.5.1
+ üõ£Ô∏è path spark: /opt/spark
+ üüß IDE: IntelliJIDEA 24.1

Los anteriores son los recursos que yo utilizo y los que en este momento me han sido compatibles, no necesariamente tienen que ser los tuyos.


## Instalar los paquetes requeridos por Spark ‚≠ê.

Antes de descargar y configurar Spark, necesitamos instalar dependencias. Estos pasos incluyen la instalaci√≥n de la siguiete paqueter√≠a.

- ‚òï JDK 
- üü• Scala 
- Git

Abrimos una terminal y corremos el siguiente comando para la instalaci√≥n de las tres paqueterias a la vez:

``` javascript
sudo apt install default-jdk scala git -y
```

Una vez que el proceso este completado, **verifica que las dependencias esten instaladas** corriendo el siguiente comando:

```
java -version; javac -version; scala -version; git --version
```

Se imprimir√°n las salida con la versiones correspondientes, si la instalaci√≥n por todos las paqueter√≠as fue exitosa.

## Descargar y Configurar Spark ‚≠ê en Debian o Ubuntu.

Ahora, necesitamos descargar la versi√≥n de Spark que quieras y este disponible desde el sitio web oficial. Al momento de editar este texto la versi√≥n m√°s actualizada es _Spark 3.2.1_ (Enero-26-2022) conjuntamente con la paqueter√≠a _Hadoop 3.2_.

Usamos el comando **wget** junto con la liga del sitio  para la descarga de nuestro archivo Spark:

```
$ wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
```

***

**Nota:** Si la URL no corre, por favor dirigete a la p√°gina oficial [Apache Spark](https://spark.apache.org/) y busca la versi√≥n m√°s reciente en el men√∫ descargas (Download). En otras palabras, tambi√©n puedes intentar remplazar las versiones en la liga que te estoy compartiendo.
***

Ahora, extraemos el archivo guardado usando _tar:_

```
$ tar xvf spark-*
```

Deje que el proceso se complete. La salida muestra los ficheros que se est√°n desempaquetando desde el archivo.

Finalmente, movemos el directorio desempaquetado spark-3.2.1-bin-hadoop3.2 al directorio _**opt/spark**_.

Usando el comando **mv** para cortar y pegar: 

``` javascript
sudo mv spark-3.2.1-bin-hadoop3.2 /opt/spark
```

## Configurar el entorno Spark ‚≠ê.

Antes de inicializar el servidor maestro, necesitamos configurar las variables de entorno. Estas son a menudo rutas (paths) en el Spark que necesitamos agregar al perfil de usuario.

Usando el comando **echo** agregamos las siguientes tres lines al _.profile:_

``` javascript
echo "export SPARK_HOME=/opt/spark" >> ~/.profile
echo "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin" >> ~/.profile
echo "export PYSPARK_PYTHON=/usr/bin/python3" >> ~/.profile
```

Tambi√©n podemos agregar las rutas de exportaci√≥n editando el fichero _.profile_ en el editor que eligas, como nano o vim.

Por ejemplo, para el editor nano, ingresamos:

```
$ nano ~/.profile
```

Cuando carge profile, nos posecionamos al final del archivo:

![](https://raw.githubusercontent.com/gabrielfernando01/spark/master/image/export_paths.png)

Entonces, agregamos las siguientes tres l√≠neas:

```
$ export SPARK_HOME=/opt/spark
$ export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
$ export PYSPARK_PYTHON=/usr/bin/python3
```

Guardamos, confirmamos y salimos.

Cuando hayamos finalizado de agregarar las rutas (paths), cargamos el fichero _.profile_ escribiendo sobre la l√≠nea de comando:

```
$ source ~/.profile
```

## Inicializando Standalone Servidor Maestro Spark

Ahora que hemos completado la configuraci√≥n del entorno Spark, podemos inicializar el servidor maestro.

En la terminal, escribimos:

```
$ start-master.sh
```

Para visualizar la interfaz web Spark, abrimos un navegador web y entramos a la direcci√≥n localhost IP sobre el puerto 8080.

```
http://127.0.0.1:8080/
```

La p√°gina muestra tu **URL Spark**, la informaci√≥n para el status de trabajadores (workers), recursos del hardware utilizados, etc.

![](https://raw.githubusercontent.com/gabrielfernando01/spark/master/image/url.png)

La URL para el Maestro Spark es el nombre de tu dispositivo sobre el puerto 8080. Para mi caso es _**debian.gabi:8080**_. As√≠ es que, aqu√≠ tenemos tres caminos posibles para cargar la Web UI Spark Master:

1. 127.0.0.1:8080
2. localhost:8080
3. deviceName:8080

## Inicializar el servidor trabajador Spark (start a worker process)

En esta configuraci√≥n standalone de un solo servidor, inicializarremos un servidor trabajador conjuntamente con el servidor maestro.

Para esto, corremos el siguiente comando en este formato:

```
$ start-slave.sh spark://master:port
```

El **master** en este comando puede ser una IP o un hostname.

En mi caso es **debian.gabi:**

```
start-slave.sh spark://debian.gabi:7077
```

Ahora que el trabajador o esclavo esta cargado y corriendo, si recargamos la Spark Master's Web UI, deber√°s de verlo en la lista:

![](https://raw.githubusercontent.com/gabrielfernando01/spark/master/image/slave.png)

### Especificar la asignaci√≥n de recursos para los trabajadores

La configuraci√≥n por defecto cuando inicializamos un trabajador sobre una m√°quinas es la disponible por todos los n√∫cleos CPU. Puedes especificar el n√∫mero de n√∫cleos que pasan por las **-c** banderas al comando **start-slave**.

```
$ start-slave.sh -c 1 spark://debian.gabi:7077
```

Recargamos el Spark Master's Web UI para confirmar la configuraci√≥n del trabajador.

![](https://raw.githubusercontent.com/gabrielfernando01/spark/master/image/core.png)

De manera similar podemos asignar la cantidad especifica de memoria cuando inicializamos al trabajador. La configuraci√≥n por defecto es la cantidad de memoria RAM usada por la m√°quina menos 1GB.

Al iniciar un trabajador y asignarle una cantidad especifica de memoria, agregamos la opci√≥n **-m** y el n√∫mero. Para gigabytes, usamos **G** y para megabytes, usamos **M.**

Por ejemplo, para iniciar un trabajador con 512MB de memoria, damos enter al siguiente comando:

```
$ start-slave.sh -m 512MB spark://debian.gabi:7077
```

Recarga el Spark Master's Web UI para visualizar el status del trabajador y confirmar la configuraci√≥n 

![](https://raw.githubusercontent.com/gabrielfernando01/spark/master/image/memory.png)

## Probar Spark Shell

Despu√©s de haber terminado de configurar y arrancar el servidor amo y esclavo, probamos si la Spark shell trabaja.

Cargamos la shell ingresando:

```
$ spark-shell
```

Deber√°s obtener una notificaci√≥n en pantalla y la descripci√≥n de Spark. Por defecto la interfaz es Scala, entonces cargara la shell cuando corras _spark-shell_.

Al finalizar la salida lucira la imagen con la versi√≥n que use al momento de escribir esta guia:

![](https://raw.githubusercontent.com/gabrielfernando01/spark/master/image/spark_shell.png)

## Probando Python üêç en Spark ‚≠ê.	

Si no quieres usar la interfaz Scala por defecto, puedes usar Python.

Asegurate de salir de Scala y entonces ingresamos el siguiente comando:

```
$ pyspark
```

La salida resultante luce similar a la anterior. En la parte inferior observar√°s la versi√≥n de Python.

![](https://raw.githubusercontent.com/gabrielfernando01/spark/master/image/pyspark.png)

Para salir de esta shell, ingresa **quit()** y oprime enter.

## Comandos basicos para iniciar y detener el Servidor Amo y Escalavos (Master and Workers Server) 

Enseguida encotrar√°s los comandos basicos para arrancar y parar el servidor amo y esclavo de Apache Spark. Esta confiugraci√≥n es para una sola m√°quina, los scripts corren por defecto en el localhost.

**Para iniciar una instancia del servidor mastestro** en la m√°quina actual, ejecutamos el comando que habiamos ya habiamos usado:

```
$ start-master.sh
```

**Para detener la instancia maestro** empezamos por ejecutar el sigueinte script, ejecutamos:

```
$ stop-master.sh
```

**Para detener un esclavo** que se esta ejecutando, ingresamos el siguiente comando:

```
$ stop-slave.sh
```

En la Web UI Spark Master, mostrara en el campo 'status' del Worker Id como DEAD.

Puedes **iniciar ambas instancias maestro y esclavo** usando el comando start-all:

```
$ start-all.sh
```

### Conclusi√≥n

Este tutorial muestra **como instalar Spark sobre una distribuci√≥n Debian y sus derivadas**, como sus dependencias necesarias.
